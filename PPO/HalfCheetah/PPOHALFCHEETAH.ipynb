{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5M5CWLfri9mglkxoZKeSO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VwJJDV1zWcGd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"id":"jQohBUdfWdZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gymnasium[mujoco]"],"metadata":{"id":"hCO4mQC-Wc25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: create DDPG_2M folder and change location to it\n","\n","import os\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists('/content/drive/My Drive/PPO_HALF_CHEETAH'):\n","    os.makedirs('/content/drive/My Drive/PPO_HALF_CHEETAH')\n","\n","# Change the current working directory\n","os.chdir('/content/drive/My Drive/PPO_HALF_CHEETAH')\n","\n","# Verify the change (optional)\n","print(os.getcwd())"],"metadata":{"id":"UlY9gYmsWeQ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import csv\n","\n","# Set the device to CPU or GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Actor-Critic Neural Network\n","class ActorCritic(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_sizes, activation):\n","        super(ActorCritic, self).__init__()\n","        # Shared layers\n","        layers = []\n","        input_dim = state_dim\n","        for hidden_size in hidden_sizes:\n","            layers.append(nn.Linear(input_dim, hidden_size))\n","            layers.append(activation())\n","            input_dim = hidden_size\n","        self.shared = nn.Sequential(*layers)\n","\n","        # Policy head\n","        self.policy_mean = nn.Linear(hidden_sizes[-1], action_dim)\n","        self.policy_std = nn.Parameter(torch.ones(action_dim) * 0.1)  # Initialize with small std dev\n","\n","        # Value head\n","        self.value = nn.Linear(hidden_sizes[-1], 1)\n","\n","    def forward(self, state):\n","        x = self.shared(state)\n","        mean = self.policy_mean(x)\n","        std = torch.exp(self.policy_std)\n","        value = self.value(x)\n","        return mean, std, value\n","\n","    def act(self, state):\n","        mean, std, _ = self.forward(state)\n","        dist = torch.distributions.Normal(mean, std)\n","        action = dist.sample()\n","        action_log_prob = dist.log_prob(action).sum(dim=-1)\n","        return action, action_log_prob\n","\n","    def evaluate(self, states, actions):\n","        mean, std, values = self.forward(states)\n","        dist = torch.distributions.Normal(mean, std)\n","        action_log_probs = dist.log_prob(actions).sum(dim=-1)\n","        entropy = dist.entropy().sum(dim=-1)\n","        return action_log_probs, values, entropy\n","\n","\n","# PPO Agent\n","class PPOAgent:\n","    def __init__(self, state_dim, action_dim, hidden_sizes, activation, lr, gamma, lam, clip_eps, update_epochs, batch_size):\n","        self.gamma = gamma\n","        self.lam = lam\n","        self.clip_eps = clip_eps\n","        self.update_epochs = update_epochs\n","        self.batch_size = batch_size\n","\n","        self.actor_critic = ActorCritic(state_dim, action_dim, hidden_sizes, activation).to(device)\n","        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n","\n","        self.buffer = []\n","\n","    def store_transition(self, transition):\n","        self.buffer.append(transition)\n","\n","    def compute_gae(self, rewards, values, dones):\n","        advantages = []\n","        returns = []\n","        gae = 0\n","        next_value = 0\n","        for step in reversed(range(len(rewards))):\n","            delta = rewards[step] + self.gamma * next_value * (1 - dones[step]) - values[step]\n","            gae = delta + self.gamma * self.lam * gae * (1 - dones[step])\n","            advantages.insert(0, gae)\n","            next_value = values[step]\n","            returns.insert(0, gae + values[step])\n","        return advantages, returns\n","\n","    def train(self):\n","        states, actions, rewards, dones, log_probs, values = zip(*self.buffer)\n","        self.buffer = []\n","\n","        states = torch.FloatTensor(np.array(states)).to(device)\n","        actions = torch.FloatTensor(np.array(actions)).to(device)\n","        rewards = torch.FloatTensor(np.array(rewards)).to(device)\n","        dones = torch.FloatTensor(np.array(dones)).to(device)\n","        old_log_probs = torch.FloatTensor(np.array(log_probs)).to(device)\n","        values = torch.FloatTensor(np.array(values)).to(device)\n","\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)  # Normalize rewards\n","        advantages, returns = self.compute_gae(rewards.cpu().numpy(), values.cpu().numpy(), dones.cpu().numpy())\n","        advantages = torch.FloatTensor(advantages).to(device)\n","        returns = torch.FloatTensor(returns).to(device)\n","\n","        for _ in range(self.update_epochs):\n","            for idx in range(0, len(states), self.batch_size):\n","                batch_indices = slice(idx, idx + self.batch_size)\n","                batch_states = states[batch_indices]\n","                batch_actions = actions[batch_indices]\n","                batch_old_log_probs = old_log_probs[batch_indices]\n","                batch_advantages = advantages[batch_indices]\n","                batch_returns = returns[batch_indices]\n","\n","                new_log_probs, new_values, entropy = self.actor_critic.evaluate(batch_states, batch_actions)\n","                ratios = torch.exp(new_log_probs - batch_old_log_probs)\n","                surr1 = ratios * batch_advantages\n","                surr2 = torch.clamp(ratios, 1 - self.clip_eps, 1 + self.clip_eps) * batch_advantages\n","                policy_loss = -torch.min(surr1, surr2).mean()\n","\n","                value_loss = nn.MSELoss()(new_values.squeeze(), batch_returns)\n","                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy.mean()\n","\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=0.5)  # Gradient clipping\n","                self.optimizer.step()\n","\n","\n","# Training loop\n","def train_ppo(env, agent, max_steps, log_interval):\n","    total_steps = 0\n","    timesteps = []\n","    avg_returns = []\n","    std_errors = []\n","    episode_rewards = []\n","\n","    csv_file = \"ppo_training_results.csv\"\n","    with open(csv_file, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([\"Timesteps\", \"Average Return\", \"Standard Error\"])\n","\n","        while total_steps < max_steps:\n","            state, info = env.reset()\n","            state = torch.FloatTensor(state).to(device)\n","            episode_reward = 0\n","\n","            while True:\n","                action, log_prob = agent.actor_critic.act(state)\n","                next_state, reward, done, truncated, info = env.step(action.cpu().numpy())\n","                next_state = torch.FloatTensor(next_state).to(device)\n","                value = agent.actor_critic.forward(state)[2]\n","\n","                agent.store_transition((state.detach().cpu().numpy(),\n","                                        action.detach().cpu().numpy(),\n","                                        reward, done,\n","                                        log_prob.detach().cpu().numpy(),\n","                                        value.detach().cpu().numpy()))\n","                state = next_state\n","                episode_reward += reward\n","                total_steps += 1\n","\n","                if done or truncated:\n","                    episode_rewards.append(episode_reward)\n","                    break\n","\n","            if total_steps % log_interval == 0:\n","                agent.train()\n","\n","                avg_return = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n","                std_error = np.std(episode_rewards[-100:]) / np.sqrt(len(episode_rewards[-100:])) if len(episode_rewards) >= 100 else 0\n","                timesteps.append(total_steps)\n","                avg_returns.append(avg_return)\n","                std_errors.append(std_error)\n","\n","                print(f\"Steps: {total_steps}, Average Return: {avg_return:.2f}, Std Error: {std_error:.2f}\")\n","                writer.writerow([total_steps, avg_return, std_error])\n","\n","    return timesteps, avg_returns, std_errors\n","\n","\n","if __name__ == \"__main__\":\n","    # Fixed hyperparameters\n","    MAX_STEPS = 1000000\n","    BATCH_SIZE = 256\n","    LR = 1e-4\n","    GAMMA = 0.99\n","    LAM = 0.95\n","    CLIP_EPS = 0.2\n","    UPDATE_EPOCHS = 10\n","    HIDDEN_SIZES = [128, 128]\n","    LOG_INTERVAL = 2000\n","\n","    # Environment setup\n","    env = gym.make(\"HalfCheetah-v4\")\n","    state_dim = env.observation_space.shape[0]\n","    action_dim = env.action_space.shape[0]\n","\n","    # Create PPO agent\n","    agent = PPOAgent(\n","        state_dim=state_dim,\n","        action_dim=action_dim,\n","        hidden_sizes=HIDDEN_SIZES,\n","        activation=nn.ReLU,\n","        lr=LR,\n","        gamma=GAMMA,\n","        lam=LAM,\n","        clip_eps=CLIP_EPS,\n","        update_epochs=UPDATE_EPOCHS,\n","        batch_size=BATCH_SIZE,\n","    )\n","\n","    # Train PPO\n","    timesteps, avg_returns, std_errors = train_ppo(env, agent, MAX_STEP)\n"],"metadata":{"id":"z1wDd9lTWe4V"},"execution_count":null,"outputs":[]}]}